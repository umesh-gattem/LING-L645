{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbwgR5UdNkkm"
      },
      "source": [
        "# Transducer implementation in PyTorch\n",
        "\n",
        "*by Loren Lugosch*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBlJNKsjTtaZ"
      },
      "source": [
        "\n",
        "In this notebook, we will implement a Transducer sequence-to-sequence model for inserting missing vowels into a sentence \n",
        "\n",
        "EX: (\"W wll mplmnt sm cd.\" --> \"We will implement some code.\")\n",
        "*idea: we can change the target sentences to be specific to a domain*\n",
        "\n",
        "\n",
        "Default: (\"Hll, Wrld\" --> \"Hello, World\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-iHU02C7fAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96674b78-135b-4f2e-e980-053b7009f6e5"
      },
      "source": [
        "import torch\n",
        "import string\n",
        "import numpy as np\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "!pip install unidecode\n",
        "import unidecode\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DIRECTIONS: Aside from the given training data, find one other open-source data. \n",
        "<15 min>\n",
        "\"\"\"\n",
        "# 1. Default training data.\n",
        "!wget https://raw.githubusercontent.com/lorenlugosch/infer_missing_vowels/master/data/train/war_and_peace.txt\n",
        "!pwd\n",
        "\n",
        "# 2. Find a second training dataset.\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "--2022-09-15 21:01:15--  https://raw.githubusercontent.com/lorenlugosch/infer_missing_vowels/master/data/train/war_and_peace.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3196229 (3.0M) [text/plain]\n",
            "Saving to: ‘war_and_peace.txt.1’\n",
            "\n",
            "war_and_peace.txt.1 100%[===================>]   3.05M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-09-15 21:01:15 (36.4 MB/s) - ‘war_and_peace.txt.1’ saved [3196229/3196229]\n",
            "\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTfRgwxmjv1B"
      },
      "source": [
        "# Building blocks\n",
        "\n",
        "First, we will define the encoder, predictor, and joiner using standard neural nets.\n",
        "\n",
        "<img src=\"https://lorenlugosch.github.io/images/transducer/transducer-model.png\" width=\"25%\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7mLFyUG7kJH"
      },
      "source": [
        "\"\"\"\n",
        "RHETORIAL Q: How does changing these numbers affect the performance of the model?\n",
        "In training? In testing? In after-paper performance?\n",
        "\"\"\"\n",
        "NULL_INDEX = 0\n",
        "\n",
        "encoder_dim = 1024\n",
        "predictor_dim = 1024\n",
        "joiner_dim = 1024"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MABMTjrGY4vz"
      },
      "source": [
        "The encoder is any network that can take as input a variable-length sequence: so, RNNs, CNNs, and self-attention/Transformer encoders will all work.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE7j2T5EY33-"
      },
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, num_inputs):\n",
        "    \"\"\"\n",
        "    @num_inputs: the input size/length\n",
        "\n",
        "    DIRECTIONS: complete the variables for the input_size, hidden_size, and bidirectional arguments in self.rnn\n",
        "    <3 min>\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    hidden_size = 2048\n",
        "    bidirectional = True\n",
        "\n",
        "\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embed = torch.nn.Embedding(num_inputs, encoder_dim)\n",
        "    self.rnn = torch.nn.GRU(input_size=encoder_dim, hidden_size=hidden_size, num_layers=3, batch_first=True, bidirectional=True, dropout=0.1)\n",
        "    self.linear = torch.nn.Linear(encoder_dim*2, joiner_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = x\n",
        "    out = self.embed(out)\n",
        "    out = self.rnn(out)[0]\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRknN6QRY9-g"
      },
      "source": [
        "The predictor is any _causal_ network (= can't look at the future): in other words, unidirectional RNNs, causal convolutions, or masked self-attention. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPARF5LmY7-r"
      },
      "source": [
        "class Predictor(torch.nn.Module):\n",
        "  def __init__(self, num_outputs):\n",
        "    \"\"\"\n",
        "    @num_outputs: the output size/length\n",
        "\n",
        "    DIRECTIONS: complete the variables for the input_size and hidden_size arguments in self.rnn\n",
        "                complete the arguments to torch.nn.Linear in self.linear (hint: 2 required)\n",
        "    <3 min>\n",
        "    \"\"\"\n",
        "    \n",
        "    hidden_size = 2048\n",
        "    bidirectional = True\n",
        "\n",
        "    super(Predictor, self).__init__()\n",
        "    self.embed = torch.nn.Embedding(num_outputs, predictor_dim)\n",
        "    self.rnn = torch.nn.GRUCell(input_size=predictor_dim, hidden_size=hidden_size)\n",
        "    self.linear = torch.nn.Linear(predictor_dim*2, joiner_dim)\n",
        "    \n",
        "    self.initial_state = torch.nn.Parameter(torch.randn(predictor_dim))\n",
        "    self.start_symbol = NULL_INDEX # In the original paper, a vector of 0s is used; just using the null index instead is easier when using an Embedding layer.\n",
        "\n",
        "\n",
        "  def forward_one_step(self, input, previous_state):\n",
        "    \"\"\"\n",
        "    This is a helper function for the inherited forward method (since the input may vary).\n",
        "    @input: decoder input\n",
        "    @previous_state: state before passing the input through the RNN's forward method\n",
        "    \"\"\"\n",
        "    embedding = self.embed(input)\n",
        "    state = self.rnn.forward(embedding, previous_state)\n",
        "    out = self.linear(state)\n",
        "    return out, state\n",
        "\n",
        "\n",
        "  def forward(self, y):\n",
        "    \"\"\"\n",
        "    @y: tensor y\n",
        "\n",
        "    DIRECTIONS: complete the variables for batch_size and U (hint: utilize how y is formatted)\n",
        "                complete the variable in the for loop, i.e. replace the 'None'\n",
        "    <5 min>\n",
        "    \"\"\"\n",
        "    batch_size = y.shape[0]\n",
        "    print(len(y), y.shape)\n",
        "    U = len(y)\n",
        "    outs = []\n",
        "    state = torch.stack([self.initial_state] * batch_size).to(y.device)\n",
        "    for u in range(U): # hint: we want to get the NULL output for the final timestep \n",
        "      if u == 0:\n",
        "        decoder_input = torch.tensor([self.start_symbol] * batch_size).to(y.device)\n",
        "      else:\n",
        "        decoder_input = y[:,u-1]\n",
        "      out, state = self.forward_one_step(decoder_input, state)\n",
        "      outs.append(out)\n",
        "    out = torch.stack(outs, dim=1)\n",
        "    return out"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHPZ3PATZEAW"
      },
      "source": [
        "The joiner is a feedforward network/MLP with one hidden layer applied independently to each $(t,u)$ index.\n",
        "\n",
        "(The linear part of the hidden layer is contained in the encoder and predictor, so we just do the nonlinearity here and then the output layer.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlzca1orZDLa"
      },
      "source": [
        "class Joiner(torch.nn.Module):\n",
        "  def __init__(self, num_outputs):\n",
        "    \"\"\"\n",
        "    @num_outputs: size of softmax output over all labels\n",
        "    \"\"\"\n",
        "    super(Joiner, self).__init__()\n",
        "    self.linear = torch.nn.Linear(joiner_dim, num_outputs)\n",
        "\n",
        "  def forward(self, encoder_out, predictor_out):\n",
        "    \"\"\"\n",
        "    @encoder_out: \n",
        "    @predictor_out: \n",
        "\n",
        "    DIRECTIONS:    choose and apply a nonlinear function of your choice\n",
        "    RHETORICAL Q:  why do we add nonlinearity in a neural network?\n",
        "    <5 min>\n",
        "    \"\"\"\n",
        "    print(encoder_out.shape, predictor_out.shape)\n",
        "    out = encoder_out + predictor_out\n",
        "    out = torch.nn.Dropout(p=0.2)(out)\n",
        "    print(\"out\", out)\n",
        "    # out = \n",
        "    # out = ___ # add nonlinearity here\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_-INbhSTApv"
      },
      "source": [
        "# Transducer model + loss function\n",
        "\n",
        "Using the encoder, predictor, and joiner, we will implement the Transducer model and its loss function.\n",
        "\n",
        "<img src=\"https://lorenlugosch.github.io/images/transducer/forward-messages.png\" width=\"25%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdcKwA_lkzxJ"
      },
      "source": [
        "We can use a simple PyTorch implementation of the loss function, relying on automatic differentiation to give us gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYSagKi-gHM4"
      },
      "source": [
        "class Transducer(torch.nn.Module):\n",
        "  def __init__(self, num_inputs, num_outputs):\n",
        "    super(Transducer, self).__init__()\n",
        "    self.encoder = Encoder(num_inputs)\n",
        "    self.predictor = Predictor(num_outputs)\n",
        "    self.joiner = Joiner(num_outputs)\n",
        "\n",
        "    if torch.cuda.is_available(): self.device = \"cuda:0\"\n",
        "    else: self.device = \"cpu\"\n",
        "    self.to(self.device)\n",
        "\n",
        "  def compute_forward_prob(self, joiner_out, T, U, y):\n",
        "    \"\"\"\n",
        "    @joiner_out: tensor of shape (B, T_max, U_max+1, num_labels)\n",
        "    @T: list of input lengths\n",
        "    @U: list of output lengths \n",
        "    @y: label tensor (B, U_max+1)\n",
        "\n",
        "    DIRECTIONS: draw out a couple iterations of the nested for loop below\n",
        "    <15 min>\n",
        "    \"\"\"\n",
        "    B = joiner_out.shape[0]                                        #B = batch size??\n",
        "    T_max = joiner_out.shape[1]\n",
        "    U_max = joiner_out.shape[2] - 1\n",
        "    log_alpha = torch.zeros(B, T_max, U_max+1).to(model.device)\n",
        "    for t in range(T_max):\n",
        "      for u in range(U_max+1):\n",
        "          if u == 0:\n",
        "            if t == 0:\n",
        "              log_alpha[:, t, u] = 0.\n",
        "\n",
        "            else: #t > 0\n",
        "              log_alpha[:, t, u] = log_alpha[:, t-1, u] + joiner_out[:, t-1, 0, NULL_INDEX] \n",
        "                  \n",
        "          else: #u > 0\n",
        "            if t == 0:\n",
        "              log_alpha[:, t, u] = log_alpha[:, t,u-1] + torch.gather(joiner_out[:, t, u-1], dim=1, index=y[:,u-1].view(-1,1) ).reshape(-1)\n",
        "            \n",
        "            else: #t > 0\n",
        "              log_alpha[:, t, u] = torch.logsumexp(torch.stack([\n",
        "                  log_alpha[:, t-1, u] + joiner_out[:, t-1, u, NULL_INDEX],\n",
        "                  log_alpha[:, t, u-1] + torch.gather(joiner_out[:, t, u-1], dim=1, index=y[:,u-1].view(-1,1) ).reshape(-1)\n",
        "              ]), dim=0)\n",
        "    \n",
        "    log_probs = []\n",
        "    for b in range(B):\n",
        "      log_prob = log_alpha[b, T[b]-1, U[b]] + joiner_out[b, T[b]-1, U[b], NULL_INDEX]\n",
        "      log_probs.append(log_prob)\n",
        "    log_probs = torch.stack(log_probs) \n",
        "    return log_prob # history of logits??\n",
        "\n",
        "  def compute_loss(self, x, y, T, U):\n",
        "    \"\"\"\n",
        "    @x: input/training tensor\n",
        "    @y: label tensor\n",
        "    @T: list of the length of input sequences\n",
        "    @U: list of the length of output sequences\n",
        "    \"\"\"\n",
        "    encoder_out = self.encoder.forward(x)\n",
        "    predictor_out = self.predictor.forward(y)\n",
        "    joiner_out = self.joiner.forward(encoder_out.unsqueeze(2), predictor_out.unsqueeze(1)).log_softmax(3)\n",
        "    loss = -self.compute_forward_prob(joiner_out, T, U, y).mean()\n",
        "    return loss"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK0c2S2xaARd"
      },
      "source": [
        "Let's first verify that the forward algorithm actually correctly computes the sum (in log space, the [logsumexp](https://lorenlugosch.github.io/posts/2020/06/logsumexp/)) of all possible alignments, using a short input/output pair for which computing all possible alignments is feasible.\n",
        "\n",
        "<img src=\"https://lorenlugosch.github.io/images/transducer/cat-align-1.png\" width=\"25%\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWtkoXH6U8Pm"
      },
      "source": [
        "def compute_single_alignment_prob(self, encoder_out, predictor_out, T, U, z, y):\n",
        "    \"\"\"\n",
        "    Computes the probability of one alignment, z.\n",
        "    @encoder_out: Transducer's self.encoder\n",
        "    @predictor_out: Transducer's self.predictor\n",
        "    @T: list of the length of input sequences\n",
        "    @U: list of the length of output sequences\n",
        "    @z: \n",
        "    @y: label tensor\n",
        "\n",
        "    DIRECTIONS: write a brief description of the argument 'z' above\n",
        "                complete the variables for t_indices and u_indices\n",
        "    <5 min>\n",
        "    \"\"\"\n",
        "    t = 0; u = 0\n",
        "    t_u_indices = []\n",
        "    y_expanded = []\n",
        "    for step in z:\n",
        "      t_u_indices.append((t,u))\n",
        "      if step == 0: # right (null)\n",
        "        y_expanded.append(NULL_INDEX)\n",
        "        t += 1\n",
        "      if step == 1: # down (label)\n",
        "        y_expanded.append(y[u])\n",
        "        u += 1\n",
        "    t_u_indices.append((T-1,U))\n",
        "    y_expanded.append(NULL_INDEX)\n",
        "\n",
        "    t_indices = range(T)\n",
        "    u_indices = range(U+1)\n",
        "    encoder_out_expanded = encoder_out[t_indices]\n",
        "    predictor_out_expanded = predictor_out[u_indices]\n",
        "    joiner_out = self.joiner.forward(encoder_out_expanded, predictor_out_expanded).log_softmax(1)\n",
        "    logprob = -torch.nn.functional.nll_loss(input=joiner_out, target=torch.tensor(y_expanded).long().to(self.device), reduction=\"sum\")\n",
        "    return logprob\n",
        "\n",
        "Transducer.compute_single_alignment_prob = compute_single_alignment_prob"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xzM0dZfea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "outputId": "93615ab0-de4d-4151-d0c5-74479422735e"
      },
      "source": [
        "# Generate example inputs/outputs\n",
        "num_outputs = len(string.ascii_uppercase) + 1 # [null, A, B, ... Z]\n",
        "model = Transducer(1, num_outputs)\n",
        "y_letters = \"CAT\"\n",
        "y = torch.tensor([string.ascii_uppercase.index(l) + 1 for l in y_letters]).unsqueeze(0).to(model.device)\n",
        "T = torch.tensor([4]); U = torch.tensor([len(y_letters)]); B = 1\n",
        "\n",
        "encoder_out = torch.randn(B, T, joiner_dim).to(model.device)\n",
        "predictor_out = torch.randn(B, U+1, joiner_dim).to(model.device)\n",
        "print(\"hey\", encoder_out.unsqueeze(2).shape)\n",
        "print(\"hello\", encoder_out.unsqueeze(1).shape)\n",
        "joiner_out = model.joiner.forward(encoder_out.unsqueeze(2), predictor_out.unsqueeze(1)).log_softmax(3)\n",
        "\n",
        "#######################################################\n",
        "# Compute loss by enumerating all possible alignments #\n",
        "#######################################################\n",
        "all_permutations = list(itertools.permutations([0]*(T-1) + [1]*U))\n",
        "all_distinct_permutations = list(Counter(all_permutations).keys())\n",
        "alignment_probs = []\n",
        "for z in all_distinct_permutations:\n",
        "  print(encoder_out[0].shape, predictor_out[0].shape)\n",
        "  alignment_prob = model.compute_single_alignment_prob(encoder_out[0], predictor_out[0], T.item(), U.item(), z, y[0])\n",
        "  alignment_probs.append(alignment_prob)\n",
        "loss_enumerate = -torch.tensor(alignment_probs).logsumexp(0)\n",
        "\n",
        "#######################################################\n",
        "# Compute loss using the forward algorithm            #\n",
        "#######################################################\n",
        "loss_forward = -model.compute_forward_prob(joiner_out, T, U, y)\n",
        "\n",
        "print(\"Loss computed by enumerating all possible alignments: \", loss_enumerate)\n",
        "print(\"Loss computed using the forward algorithm: \", loss_forward)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey torch.Size([1, 4, 1, 1024])\n",
            "hello torch.Size([1, 1, 4, 1024])\n",
            "torch.Size([1, 4, 1, 1024]) torch.Size([1, 1, 4, 1024])\n",
            "out tensor([[[[-1.2812,  0.2049,  0.0000,  ...,  1.2760, -0.3664, -3.2343],\n",
            "          [-0.0000,  0.6292,  3.0060,  ..., -0.0000, -1.7359,  0.0000],\n",
            "          [ 0.2681,  0.5716,  0.0000,  ...,  3.5687, -0.6983, -0.0000],\n",
            "          [ 1.3766,  0.3549,  3.7600,  ..., -1.0130,  0.5102, -0.0330]],\n",
            "\n",
            "         [[ 0.0000, -0.8644, -0.8802,  ...,  0.1923, -1.4247, -1.0406],\n",
            "          [ 1.9083, -0.4401,  1.3181,  ..., -1.7208, -0.0000,  2.4114],\n",
            "          [ 2.2493, -0.0000,  1.3780,  ...,  2.4850, -1.7566,  1.6380],\n",
            "          [ 0.0000, -0.7144,  2.0721,  ..., -2.0968, -0.5481,  2.1607]],\n",
            "\n",
            "         [[-1.1801, -1.0613,  1.3884,  ..., -0.4555, -1.6936, -3.4586],\n",
            "          [ 0.0282, -0.6369,  3.5866,  ..., -2.3686, -0.0000, -0.0065],\n",
            "          [ 0.0000, -0.6946,  0.0000,  ...,  1.8372, -2.0255, -0.7800],\n",
            "          [ 1.4777, -0.9113,  4.3407,  ..., -2.7446, -0.8170, -0.2573]],\n",
            "\n",
            "         [[ 0.2305,  0.4935,  0.0173,  ...,  0.0000, -0.8865, -0.0000],\n",
            "          [ 0.0000,  0.9178,  2.2155,  ..., -0.7252, -0.0000,  2.8115],\n",
            "          [ 0.0000,  0.8602,  2.2755,  ...,  3.4806, -0.0000,  2.0381],\n",
            "          [ 2.8883,  0.0000,  2.9696,  ..., -0.0000, -0.0000,  2.5607]]]],\n",
            "       device='cuda:0')\n",
            "torch.Size([4, 1024]) torch.Size([4, 1024])\n",
            "torch.Size([4, 1024]) torch.Size([4, 1024])\n",
            "out tensor([[-0.0000,  0.2049,  0.8077,  ...,  1.2760, -0.3664, -3.2343],\n",
            "        [ 1.9083, -0.4401,  1.3181,  ..., -1.7208, -2.7942,  2.4114],\n",
            "        [ 0.3691, -0.0000,  3.6466,  ...,  1.8372, -2.0255, -0.7800],\n",
            "        [ 2.8883,  0.6435,  2.9696,  ..., -0.0000, -0.0099,  2.5607]],\n",
            "       device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-458b2296ee5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_distinct_permutations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0malignment_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_single_alignment_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0malignment_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignment_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mloss_enumerate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignment_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-94-4e952f643aef>\u001b[0m in \u001b[0;36mcompute_single_alignment_prob\u001b[0;34m(self, encoder_out, predictor_out, T, U, z, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mpredictor_out_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mjoiner_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoiner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_out_expanded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_out_expanded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mlogprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoiner_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_expanded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogprob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2688\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2689\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (4) to match target batch_size (7)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSBAwQONf3z9"
      },
      "source": [
        "Now let's add the greedy search algorithm for predicting an output sequence.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "(Note that I've assumed we're using RNNs for the predictor here. You would have to modify this code a bit if you want to use convolutions/self-attention instead.) \n",
        "<br/><br/>\n",
        "<img src=\"https://lorenlugosch.github.io/images/transducer/greedy-search.png\" width=\"50%\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0xeyb7Jf18_"
      },
      "source": [
        "\"\"\"\n",
        "DIRECTIONS: YOU DO NOT NEED TO IMPLEMENT BEAM SEARCH\n",
        "Here is an *opportunity* to create a beam-search. While the code\n",
        "for a greedy search is here, we can improve this algorithmically! So, you\n",
        "use the greedy search code here to ensure that things are working\n",
        "\n",
        "<might take a while>\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "def greedy_search(self, x, T):\n",
        "  y_batch = []\n",
        "  B = len(x)\n",
        "  encoder_out = self.encoder.forward(x)\n",
        "  U_max = 200\n",
        "  for b in range(B):\n",
        "    t = 0; u = 0; y = [self.predictor.start_symbol]; predictor_state = self.predictor.initial_state.unsqueeze(0)\n",
        "    while t < T[b] and u < U_max:\n",
        "      predictor_input = torch.tensor([ y[-1] ]).to(x.device)\n",
        "      g_u, predictor_state = self.predictor.forward_one_step(predictor_input, predictor_state)\n",
        "      f_t = encoder_out[b, t]\n",
        "      h_t_u = self.joiner.forward(f_t, g_u)\n",
        "      argmax = h_t_u.max(-1)[1].item()\n",
        "      if argmax == NULL_INDEX:\n",
        "        t += 1\n",
        "      else: # argmax == a label\n",
        "        u += 1\n",
        "        y.append(argmax)\n",
        "    y_batch.append(y[1:]) # remove start symbol\n",
        "  return y_batch\n",
        "\n",
        "Transducer.greedy_search = greedy_search\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhUQMJ-23f2y",
        "outputId": "85b406cd-8cfd-4c8c-817b-0cb851531f56"
      },
      "source": [
        "!pip install speechbrain\n",
        "from speechbrain.nnet.loss.transducer_loss import TransducerLoss\n",
        "transducer_loss = TransducerLoss(0)\n",
        "\n",
        "def compute_loss(self, x, y, T, U):\n",
        "    encoder_out = self.encoder.forward(x)\n",
        "    predictor_out = self.predictor.forward(y)\n",
        "    joiner_out = self.joiner.forward(encoder_out.unsqueeze(2), predictor_out.unsqueeze(1)).log_softmax(3)\n",
        "    #loss = -self.compute_forward_prob(joiner_out, T, U, y).mean()\n",
        "    T = T.to(joiner_out.device)\n",
        "    U = U.to(joiner_out.device)\n",
        "    loss = transducer_loss(joiner_out, y, T, U) #, blank_index=NULL_INDEX, reduction=\"mean\")\n",
        "    return loss\n",
        "\n",
        "Transducer.compute_loss = compute_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting speechbrain\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a0/16623a27ceddd5c96dbf1f0ce3393c5266a8276a805f1a111382f8a46efb/speechbrain-0.5.5-py3-none-any.whl (350kB)\n",
            "\r\u001b[K     |█                               | 10kB 22.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 29.3MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 23.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 26.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 51kB 25.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61kB 27.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 81kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 92kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 102kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 112kB 17.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 122kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 133kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 153kB 17.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 163kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 174kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 184kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 194kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 204kB 17.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 215kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 225kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 235kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 245kB 17.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 256kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 266kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 276kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 286kB 17.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 296kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 307kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 317kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 327kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 337kB 17.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 348kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 17.7MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from speechbrain) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from speechbrain) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from speechbrain) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from speechbrain) (1.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from speechbrain) (1.8.1+cu101)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 52.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from speechbrain) (20.9)\n",
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 50.6MB/s \n",
            "\u001b[?25hCollecting hyperpyyaml\n",
            "  Downloading https://files.pythonhosted.org/packages/17/e2/63e6353151cb4359f66e93f52152d7d60c1b32c87f5b2e2e58419d2a3711/HyperPyYAML-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->speechbrain) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->speechbrain) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->speechbrain) (2.4.7)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 49.0MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml>=0.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/4e/c3105bbbbc662f6a671a505f00ec771e93b5254f09fbb06002af9087071a/ruamel.yaml-0.17.4-py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->speechbrain) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->speechbrain) (3.4.1)\n",
            "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.10\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/6e/f652c56bbb2c3d3fca252ffc7c0358597f57a1bbdf484dac683054950c63/ruamel.yaml.clib-0.2.2-cp37-cp37m-manylinux1_x86_64.whl (547kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 57.2MB/s \n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, sentencepiece, torchaudio, pyyaml, ruamel.yaml.clib, ruamel.yaml, hyperpyyaml, speechbrain\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.8 hyperpyyaml-1.0.0 pyyaml-5.4.1 ruamel.yaml-0.17.4 ruamel.yaml.clib-0.2.2 sentencepiece-0.1.95 speechbrain-0.5.5 torchaudio-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff9raB0jVGzN"
      },
      "source": [
        "# Some utilities\n",
        "\n",
        "Here we will add a bit of boilerplate code for training and loading data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b17OQm4WdVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25498f94-9543-40f8-dd1e-fbfd7857debf"
      },
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, lines, batch_size):\n",
        "    \"\"\"\n",
        "    @lines: list of strings\n",
        "    \"\"\"\n",
        "    lines = list(filter((\"\\n\").__ne__, lines))\n",
        "\n",
        "    self.lines = lines \n",
        "    collate = Collate()\n",
        "    self.loader = torch.utils.data.DataLoader(self, batch_size=batch_size, num_workers=1, shuffle=True, collate_fn=collate)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.lines)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    line = self.lines[idx].replace(\"\\n\", \"\")\n",
        "    line = unidecode.unidecode(line) # remove special characters\n",
        "    x = \"\".join(c for c in line if c not in \"AEIOUaeiou\") # remove vowels from input\n",
        "    y = line\n",
        "    return (x,y)\n",
        "\n",
        "def encode_string(s):\n",
        "  \"\"\"\n",
        "  @s: string\n",
        "  \"\"\"\n",
        "  for c in s:\n",
        "    if c not in string.printable:\n",
        "      print(s)\n",
        "  return [string.printable.index(c) + 1 for c in s]\n",
        "\n",
        "def decode_labels(l):\n",
        "  \"\"\"\n",
        "  @l: list of labels\n",
        "  \"\"\"\n",
        "  return \"\".join([string.printable[c - 1] for c in l])\n",
        "\n",
        "\n",
        "class Collate:\n",
        "  def __call__(self, batch):\n",
        "    \"\"\"\n",
        "    Returns a minibatch of strings, encoded as labels and padded to have the same length.\n",
        "    @batch: list of tuples (input string, output string)\n",
        "\n",
        "    DIRECTIONS: after obtaining results from training on the default text, train on your second training text \n",
        "    <10 min>\n",
        "    \"\"\"\n",
        "    x = []; y = []\n",
        "    batch_size = len(batch)\n",
        "    for index in range(batch_size):\n",
        "      x_,y_ = batch[index]\n",
        "      x.append(encode_string(x_))\n",
        "      y.append(encode_string(y_))\n",
        "\n",
        "    # pad all sequences to have same length\n",
        "    T = [len(x_) for x_ in x]\n",
        "    U = [len(y_) for y_ in y]\n",
        "    T_max = max(T)\n",
        "    U_max = max(U)\n",
        "    for index in range(batch_size):\n",
        "      x[index] += [NULL_INDEX] * (T_max - len(x[index]))\n",
        "      x[index] = torch.tensor(x[index])\n",
        "      y[index] += [NULL_INDEX] * (U_max - len(y[index]))\n",
        "      y[index] = torch.tensor(y[index])\n",
        "\n",
        "    # stack into single tensor\n",
        "    x = torch.stack(x)\n",
        "    y = torch.stack(y)\n",
        "    T = torch.tensor(T)\n",
        "    U = torch.tensor(U)\n",
        "\n",
        "    return (x,y,T,U)\n",
        "\n",
        "with open(\"war_and_peace.txt\", \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "end = round(0.9 * len(lines))\n",
        "train_lines = lines[:end]\n",
        "test_lines = lines[end:]\n",
        "train_set = TextDataset(train_lines, batch_size=64) #8)\n",
        "test_set = TextDataset(test_lines, batch_size=64) #8)\n",
        "train_set.__getitem__(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('\"Wll, Prnc, s Gn nd Lcc r nw jst fmly stts f th',\n",
              " '\"Well, Prince, so Genoa and Lucca are now just family estates of the')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaZEQYzfFEQ0"
      },
      "source": [
        "class Trainer:\n",
        "  def __init__(self, model, lr):\n",
        "    self.model = model\n",
        "    self.lr = lr\n",
        "    self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
        "  \n",
        "  def train(self, dataset, print_interval = 20):\n",
        "    train_loss = 0\n",
        "    num_samples = 0\n",
        "    self.model.train()\n",
        "    pbar = tqdm(dataset.loader)\n",
        "    for idx, batch in enumerate(pbar):\n",
        "      x,y,T,U = batch\n",
        "      x = x.to(self.model.device); y = y.to(self.model.device)\n",
        "      batch_size = len(x)\n",
        "      num_samples += batch_size\n",
        "      loss = self.model.compute_loss(x,y,T,U)\n",
        "      self.optimizer.zero_grad()\n",
        "      pbar.set_description(\"%.2f\" % loss.item())\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      train_loss += loss.item() * batch_size\n",
        "      if idx % print_interval == 0:\n",
        "        self.model.eval()\n",
        "        guesses = self.model.greedy_search(x,T)\n",
        "        self.model.train()\n",
        "        print(\"\\n\")\n",
        "        for b in range(2):\n",
        "          print(\"input:\", decode_labels(x[b,:T[b]]))\n",
        "          print(\"guess:\", decode_labels(guesses[b]))\n",
        "          print(\"truth:\", decode_labels(y[b,:U[b]]))\n",
        "          print(\"\")\n",
        "    train_loss /= num_samples\n",
        "    return train_loss\n",
        "\n",
        "  def test(self, dataset, print_interval=1):\n",
        "    test_loss = 0\n",
        "    num_samples = 0\n",
        "    self.model.eval()\n",
        "    pbar = tqdm(dataset.loader)\n",
        "    for idx, batch in enumerate(pbar):\n",
        "      x,y,T,U = batch\n",
        "      x = x.to(self.model.device); y = y.to(self.model.device)\n",
        "      batch_size = len(x)\n",
        "      num_samples += batch_size\n",
        "      loss = self.model.compute_loss(x,y,T,U)\n",
        "      pbar.set_description(\"%.2f\" % loss.item())\n",
        "      test_loss += loss.item() * batch_size\n",
        "      if idx % print_interval == 0:\n",
        "        print(\"\\n\")\n",
        "        print(\"input:\", decode_labels(x[0,:T[0]]))\n",
        "        print(\"guess:\", decode_labels(self.model.greedy_search(x,T)[0]))\n",
        "        print(\"truth:\", decode_labels(y[0,:U[0]]))\n",
        "        print(\"\")\n",
        "    test_loss /= num_samples\n",
        "    return test_loss\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4PupgBKWe6p"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "Now we will train a model. This will generate some output sequences every 20 batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TSrbH9xGPEC"
      },
      "source": [
        "num_chars = len(string.printable)\n",
        "model = Transducer(num_inputs=num_chars+1, num_outputs=num_chars+1)\n",
        "trainer = Trainer(model=model, lr=0.0003)\n",
        "\n",
        "num_epochs = 1\n",
        "train_losses=[]\n",
        "test_losses=[]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = trainer.train(train_set)\n",
        "    test_loss = trainer.test(test_set)\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    print(\"Epoch %d: train loss = %f, test loss = %f\" % (epoch, train_loss, test_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRahAWPoubyu"
      },
      "source": [
        "print(train_losses)\n",
        "print(test_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLQKw4kmFj3S"
      },
      "source": [
        "Let's test the model on a new sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhH5lYdyEazJ",
        "outputId": "d7938f4f-0f91-477c-8d87-03163c2ed7bc"
      },
      "source": [
        "\"\"\"\n",
        "DIRECTIONS: Experiment with different test outputs. What are some things to keep in mind when changing the test outputs?\n",
        "<5 min>\n",
        "\"\"\"\n",
        "\n",
        "test_output = \"Most people have little difficulty reading this sentence\"\n",
        "test_input = \"\".join(c for c in test_output if c not in \"AEIOUaeiou\")\n",
        "print(\"input: \" + test_input)\n",
        "x = torch.tensor(encode_string(test_input)).unsqueeze(0).to(model.device)\n",
        "y = torch.tensor(encode_string(test_output)).unsqueeze(0).to(model.device)\n",
        "T = torch.tensor([x.shape[1]]).to(model.device)\n",
        "U = torch.tensor([y.shape[1]]).to(model.device)\n",
        "guess = model.greedy_search(x,T)[0]\n",
        "print(\"truth: \" + test_output)\n",
        "print(\"guess: \" + decode_labels(guess))\n",
        "print(\"\")\n",
        "y_guess = torch.tensor(guess).unsqueeze(0).to(model.device)\n",
        "U_guess = torch.tensor(len(guess)).unsqueeze(0).to(model.device)\n",
        "\n",
        "print(\"NLL of truth: \" + str(model.compute_loss(x, y, T, U)))\n",
        "print(\"NLL of guess: \" + str(model.compute_loss(x, y_guess, T, U_guess)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input: Mst ppl hv lttl dffclty rdng ths sntnc\n",
            "truth: Most people have little difficulty reading this sentence\n",
            "guess: Mos pepel have litle dificulty riding thes sentenc\n",
            "\n",
            "NLL of truth: tensor(0.1161, device='cuda:0', grad_fn=<TransducerBackward>)\n",
            "NLL of guess: tensor(1.4692, device='cuda:0', grad_fn=<TransducerBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET__-ItZD8eA"
      },
      "source": [
        "Observe that the negative log-likelihood of the guess is actually worse than that of the true label sequence (AKA, a \"[search error](https://www.aclweb.org/anthology/D19-1331.pdf)\"). This suggests that we could get better results using a beam search instead of the greedy search."
      ]
    }
  ]
}